Our computational model peer review system involves verifying that the model's source code and documentation is complete and meets baseline standards that are derived from ["good enough practices"]({{build_absolute_uri("/resources/guides-to-good-practice/")}}) in the software engineering and scientific communities we serve. Through this process we hope to foster higher quality models shared in the community for reuse, reproducibility, and advancement of the field as well as serve as an exemplar in the [emerging practice of software citation](https://www.force11.org/software-citation-principles).

Reviewers are asked to review the computational model according to the following criteria:

1. Are you able to run the model? This may involve compilation into an executable, resolving input data dependencies or software library dependencies - all of which should be clearly documented by the author.
2. Is it sufficiently documented using the [ODD protocol](http://www.ufz.de/index.php?de=40429) or equivalent detailed narrative protocol? Although this can be an admittedly subjective evaluation you should be looking for cogent narrative documentation with enough detail to enable other computational modelers to replicate the model and its results without having to refer to the source code. Good documentation will often present important detailed internals and assumptions made in the model as well as a high level overview of the model.
3. Does the model have well-structured, "clean" code with appropriate comments in addition to documented inputs and expected outputs? This will help others reuse, review, or make improvements to the code.

In this review process you do not need to assess whether the model is theoretically sound, has scientific merit or is producing correct outputs. Of course, if you spot any red flags in the code please feel free to raise them in your private correspondence with the review editor.
